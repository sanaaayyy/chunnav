# -*- coding: utf-8 -*-
"""Project-Exhibition-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sj7AMmhtHB81GIqqpCt8EXWfHuVFP2Wp

# **Prediction Model** of exit polls and opinion polls in india using Linear Regression Model.
"""

from google.colab import drive
drive.mount('/content/drive')

#importing libraries
import numpy as np
import pandas as pd
import re # re module provides regular expression matching operations
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler # MinMaxScaler scales the input data by subtracting the minimum value and dividing the range (i.e., the difference between the minimum and maximum values) of each feature.
from sklearn.preprocessing import LabelEncoder, OneHotEncoder #imports the LabelEncoder and OneHotEncoder classes from the sklearn.preprocessing module.
#The LabelEncoder is used to convert categorical labels to numerical values and the OneHotEncoder is used to convert categorical integer features to a one-hot encoded representation. These classes can be used as preprocessing steps in a machine learning pipeline to convert categorical features of the input data into a numerical representation that can be used by a model.

"""The code imports several libraries that are commonly used for machine learning tasks, such as:

*   numpy for numerical computations
*   pandas for data manipulation and analysis
*   TfidfVectorizer from sklearn.feature_extraction.text for converting text data into numerical data in the form of Tf-Idf features.
*   train_test_split from sklearn.model_selection for splitting the data into training and testing sets.
*   accuracy_score from sklearn.metrics for evaluating the performance of the model by computing the accuracy score.













"""

!pip install streamlit

import streamlit as st

#importing dataset
df = pd.read_csv('/content/drive/MyDrive/Project-Exhibition-2/Dataset/Loksabha_1962-2019 .csv' , delimiter=',')
# delimiter separates the values in each row of the file into separate columns.

#Load DataFrame
df.head()

df['Pc_name'].describe()

print(df)

#Encoding or feature encoding
dict_pc_name = {}
global curr
curr = 0
def My_Encoder(idx):
  if dict_pc_name.get(idx) is not None:
    return dict_pc_name.get(idx)
  else:
    temp = len(dict_pc_name)+1
    dict_pc_name[idx] = temp
    return temp
df['New_Pc_name'] = df['Pc_name'].apply(lambda i: My_Encoder(i))

"""The code starts by defining a dictionary dict_pc_name and a global variable curr that keeps track of the number of unique values in 'Pc_name' that have been encountered so far.

The function My_Encoder is then defined. It takes an argument idx, which is a value from the 'Pc_name' column. The function first checks if idx is already a key in the dict_pc_name dictionary. If it is, the function returns the corresponding value (i.e., the encoded value). If it is not, the function creates a new key-value pair in the dictionary, where the key is idx and the value is the current value of curr plus one. The function then returns the new value.

Finally, the apply method is used to apply the My_Encoder function to each value in the 'Pc_name' column. The resulting values are stored in the new column 'New_Pc_name'.
"""

df1 = df.drop(['Pc_name','no','Turnout','margin','margin%','year'],axis=1)
# the columns Pc_name, no, Turnout, margin, margin%, and year are dropped from the original dataframe df.
# The parameter axis=1 specifies that the operation should be performed on columns.

df1.head()

df1['state'].unique()

#Data cleaning and preprocessing using regular expressions and label encoding
regex = r'(?<!\[)[^\[\]]+(?!\])'
def Changer(str1):
  matches = re.findall(regex, str1)
  temp = matches[0]
  temp = re.sub(r'^\s+|\s+$', '', temp)
  # print(matches[0])
  return temp

df1['state'] = df['state'].apply(lambda i: Changer(i))
encoder = LabelEncoder()
df1['New_State'] = encoder.fit_transform(df1['state'])
df1['New_type'] = encoder.fit_transform(df1['type'])
df1['New_candidate_name'] = encoder.fit_transform(df1['candidate_name'])
df1['New_party'] = encoder.fit_transform(df1['party'])

"""*   The Changer function uses regular expression to extract the state name from the state column in the dataframe.
*   The apply method is used to apply the Changer function to each row in the state column of the dataframe.
*   The LabelEncoder from scikit-learn is used to encode the categorical variables state, type, candidate_name, and party into numeric values.
*   The encoded variables are added to the df1 dataframe with names prefixed with "New_" to indicate they are the new encoded variables.
"""

df1.head()

df2 = df1.drop(['state','type','candidate_name','party'],axis=1)
# the columns 'state','type','candidate_name','party are dropped from the dataframe df1.

df2.head()

df2.info()

# Conversion
def Converting(i):
  i = i.replace(",","")
  try:
    i = float(i)
  except:
    i = 0
  return i

df2['votes_new'] = df2['votes'].apply(lambda i: Converting(i))
df2['electors_new'] = df2['electors'].apply(lambda i: Converting(i))

"""This code defines a function Converting which takes a string argument i. The function first replaces any commas in i with an empty string. Then, it attempts to convert the resulting string to a floating point number using the float() function. If the conversion fails due to an error, the function returns 0."""

df2.head()

df3 = df2.drop(['electors','votes'],axis=1)
# dropping electors and votes from df2.

df3.head()

df3.info()

df3.isna().sum()
# isna() function is used to detect missing values (NaN) in a DataFrame.

from sklearn.preprocessing import MinMaxScaler

# create a MinMaxScaler object
scaler = MinMaxScaler()

df3['electors_new'] = scaler.fit_transform(df3[['electors_new']])
df3['votes_new'] = scaler.fit_transform(df3[['votes_new']])

df3.head

X = df3.drop(['votes_new'],axis=1)
Y = df3['votes_new']

# train-test split method.
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)

print("The Shape of X_train:- " + str(X_train.shape))
print("The Shape of Y_train:- " + str(y_train.shape))
print("The Shape of X_test:- " + str(X_test.shape))
print("The Shape of Y_test- " + str(y_test.shape))

# Model Training
from sklearn.linear_model import LinearRegression
regressor= LinearRegression()
regressor.fit(X_train, y_train) #function estimates the coefficients of the linear regression line that best fit the training data using the Ordinary Least Squares (OLS) method.

y_pred= regressor.predict(X_test)

# Evaluating the performance of a linear regression model.
from sklearn.metrics import r2_score, mean_squared_error
print("R-squared:", r2_score(y_test, y_pred))
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))

df3.head()

from sklearn import svm, datasets
regressor.predict([[2,1,0,4036,59,0.124414]])

"""Saving the trained LinearRegression model as a binary file on disk using the pickle module."""

import pickle

filename = 'PredExitPoll.pkl'
pickle.dump(regressor, open(filename, 'wb'))

loaded_model = pickle.load(open(filename,'rb'))
loaded_model.predict(X_test)